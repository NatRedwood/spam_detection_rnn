{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impact of RNN Architecture - Code Part I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statistics as st\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from keras.layers import GRU, SimpleRNN, Embedding, Dense, LSTM, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.metrics import Precision, Recall\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "\n",
    "full_dataset = pd.read_csv(\"C:/Users/natal/CSCI5922_NN/Lab3/spam_detection_rnn/data/SPAM text message 20170820 - Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = []\n",
    "labels = []\n",
    "\n",
    "for x, label in enumerate(full_dataset['Category']):\n",
    "    full_text.append(full_dataset['Message'][x])\n",
    "    if label == 'ham':\n",
    "        labels.append(0)\n",
    "    else:\n",
    "        labels.append(1)\n",
    "\n",
    "full_text = np.asarray(full_text)\n",
    "labels = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of separate messages:  5572\n",
      "Size of labels:  5572\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of separate messages: \", len(full_text))\n",
    "print(\"Size of labels: \", len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset split - for basic full test split (all sequence lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Train size:  3900 \n",
      "Test size:  1672\n",
      "Tokens size:  9004\n",
      "Data size after tokenizing:  (5572, 500)\n",
      "Train x shape after split:  (3900, 500) \n",
      "Train y shape after split:  (3900,)\n",
      "Test x shape after split:  (1672, 500) \n",
      "Test y shape after split:  (1672,)\n"
     ]
    }
   ],
   "source": [
    "# features - number of words\n",
    "max_features = 10000\n",
    "\n",
    "# Splitting the data\n",
    "train = int(5572 * .7)\n",
    "#print(train)\n",
    "test = int(5572 - train)\n",
    "#print(test)\n",
    "\n",
    "# Checking the shapes are correct\n",
    "print(len(full_text) == (train + test))\n",
    "print(\"Train size: \", train, \"\\nTest size: \", test)\n",
    "\n",
    "# Tokenizning \n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(full_text)\n",
    "sequences = tokenizer.texts_to_sequences(full_text)\n",
    "\n",
    "# after, stop putting more words\n",
    "max_len = 500\n",
    "\n",
    "idx_word = tokenizer.word_index\n",
    "print(\"Tokens size: \", len(idx_word))\n",
    "\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# Checking the shape\n",
    "print(\"Data size after tokenizing: \", padded_sequences.shape)\n",
    "\n",
    "np.random.seed(42)\n",
    "indices = np.arange(padded_sequences.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "padded_sequences_idx = padded_sequences[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "x_train = padded_sequences_idx[:train]\n",
    "y_train = labels[:train]\n",
    "x_test = padded_sequences_idx[train:]\n",
    "y_test = labels[train:]\n",
    "\n",
    "# Checking the shapes after splitting\n",
    "print(\"Train x shape after split: \", x_train.shape, \"\\nTrain y shape after split: \", y_train.shape)\n",
    "print(\"Test x shape after split: \", x_test.shape, \"\\nTest y shape after split: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the messages' length - necessary for further splitting the test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------\n",
      "\n",
      "Sentence example from the messages:\n",
      "\n",
      " FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv\n",
      "\n",
      "------------------------------------------------------\n",
      "\n",
      "Checking the number of messages: 5572\n",
      "\n",
      "------------------------------------------------------\n",
      "\n",
      "Sentence example after tokenizing: \n",
      "\n",
      " ['FreeMsg', 'Hey', 'there', 'darling', \"it's\", 'been', '3', \"week's\", 'now', 'and', 'no', 'word', 'back!', \"I'd\", 'like', 'some', 'fun', 'you', 'up', 'for', 'it', 'still?', 'Tb', 'ok!', 'XxX', 'std', 'chgs', 'to', 'send,', '£1.50', 'to', 'rcv']\n",
      "\n",
      "------------------------------------------------------\n",
      "\n",
      "Length of the first 20 messages: \n",
      "\n",
      "[20, 6, 28, 11, 13, 32, 16, 26, 26, 29, 21, 26, 26, 37, 8, 19, 4, 19, 13, 24]\n",
      "\n",
      "------------------------------------------------------\n",
      "\n",
      "Sorted list; descending:\n",
      "\n",
      "[171, 162, 125, 125, 121, 119, 99, 96, 96, 95, 89, 88, 80, 79, 79, 79, 77, 77, 76, 73]\n",
      "\n",
      "------------------------------------------------------\n",
      "\n",
      "The mean of the first set in the desceding list (large): 28.324178782983306\n",
      "\n",
      "------------------------------------------------------\n",
      "\n",
      "The mean of the first set in the desceding list (medium): 12.775982767905223\n",
      "Lengths of 20 messages before the frist division: [19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19]\n",
      "\n",
      "------------------------------------------------------\n",
      "\n",
      "The mean of the first set in the desceding list (short): 5.889128094725511\n",
      "Lengths of 20 messages before the second division: [9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9]\n",
      "\n",
      "------------------------------------------------------\n",
      "\n",
      "Chosen boundaries for lengths:\n",
      "\n",
      "Short: 9\n",
      "Medium: 18\n",
      "Long: 170\n"
     ]
    }
   ],
   "source": [
    "# Checking the length of messages to perform the division of the test set based on the number of words\n",
    "# Creating the boundaries for: Short, medium and long inputs\n",
    "\n",
    "texts_list = full_text.tolist()\n",
    "print(\"\\n------------------------------------------------------\\n\")\n",
    "print(f\"Sentence example from the messages:\\n\\n {texts_list[5]}\")\n",
    "print(\"\\n------------------------------------------------------\\n\")\n",
    "\n",
    "def tokenizing():\n",
    "    splitted = []\n",
    "    for x in texts_list:\n",
    "        splitted.append(x.split(' '))\n",
    "    return splitted\n",
    "\n",
    "splitted_messages = tokenizing()\n",
    "print(f\"Checking the number of messages: {len(splitted_messages)}\")\n",
    "print(\"\\n------------------------------------------------------\\n\")\n",
    "print(f\"Sentence example after tokenizing: \\n\\n {splitted_messages[5]}\")\n",
    "print(\"\\n------------------------------------------------------\\n\")\n",
    "\n",
    "def counting_length():\n",
    "    lengths = []\n",
    "    for x in splitted_messages:\n",
    "        lengths.append(len(x))\n",
    "    return lengths\n",
    "\n",
    "print(f\"Length of the first 20 messages: \\n\\n{counting_length()[:20]}\")\n",
    "print(\"\\n------------------------------------------------------\\n\")\n",
    "\n",
    "sorted_counts = sorted(counting_length(), reverse=True)\n",
    "print(f\"Sorted list; descending:\\n\\n{sorted_counts[:20]}\")\n",
    "\n",
    "div = math.floor(len(texts_list)/3)\n",
    "long_lenghts = sorted_counts[:div]\n",
    "print(\"\\n------------------------------------------------------\\n\")\n",
    "print(f\"The mean of the first set in the desceding list (large): {st.mean(long_lenghts)}\")\n",
    "\n",
    "medium_lenghts = sorted_counts[div:int(2*div)]\n",
    "print(\"\\n------------------------------------------------------\\n\")\n",
    "print(f\"The mean of the first set in the desceding list (medium): {st.mean(medium_lenghts)}\")\n",
    "print(f\"Lengths of 20 messages before the frist division: {sorted_counts[div-20:int(2*div)][:20]}\")\n",
    "\n",
    "short_lenghts = sorted_counts[2*div:]\n",
    "print(\"\\n------------------------------------------------------\\n\")\n",
    "print(f\"The mean of the first set in the desceding list (short): {st.mean(short_lenghts)}\")\n",
    "print(f\"Lengths of 20 messages before the second division: {sorted_counts[(2*div-20):][:20]}\")\n",
    "print(\"\\n------------------------------------------------------\\n\")\n",
    "\n",
    "short=9\n",
    "medium=18\n",
    "long=170\n",
    "print(f\"Chosen boundaries for lengths:\\n\\nShort: {short}\\nMedium: {medium}\\nLong: {long}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data preparation for test-size experiments based on the sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------\n",
      "\n",
      "Length of short messages dataset: 1861\n",
      "\n",
      "Example of a message: 0    Yeah I am, so I'll leave maybe 7ish?\n",
      "Name: messages, dtype: object\n",
      "\n",
      "------------------------------------------------------\n",
      "\n",
      "Length of medium messages dataset: 1843\n",
      "\n",
      "Example of a message: 0    Did you get any gift? This year i didnt get anything. So bad\n",
      "Name: messages, dtype: object\n",
      "\n",
      "------------------------------------------------------\n",
      "\n",
      "Length of long messages dataset: 1868\n",
      "\n",
      "Example of a long message: 0    Collect your VALENTINE'S weekend to PARIS inc Flight & Hotel + £200 Prize guaranteed! Text: PARIS to No: 69101. www.rtf.sphosting.com\n",
      "Name: messages, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Creating a dataframe with messages and word counts\n",
    "\n",
    "#Checking the lengths of texts and labels\n",
    "len(texts_list)\n",
    "len(labels)\n",
    "\n",
    "#Making sure the numpy arrays are converted to lists\n",
    "type(texts_list)\n",
    "type(labels)\n",
    "\n",
    "labels_list = labels.tolist()\n",
    "\n",
    "type(labels_list)\n",
    "\n",
    "#Creating a dictionary and then making a dataframe \n",
    "dict_full_dataset = {'messages': texts_list, 'labels': labels_list}\n",
    "df_full_dataset = pd.DataFrame(dict_full_dataset)\n",
    "df_full_dataset.head()\n",
    "\n",
    "#Creating an additional column with word counts\n",
    "df_full_dataset['word_count'] = df_full_dataset['messages'].str.split().str.len()\n",
    "df_full_dataset\n",
    "\n",
    "#Setting df options to display example messages\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "#Creating three new dataframes based on the conditions of the length of seqeunces\n",
    "#Boundaries for nummber of words were created in the previous cells with the analysis of the dataset\n",
    "df_short_messages = df_full_dataset[df_full_dataset['word_count']<short]\n",
    "print(\"\\n------------------------------------------------------\\n\")\n",
    "print(f\"Length of short messages dataset: {len(df_short_messages)}\\n\")\n",
    "print(f\"Example of a message: {df_short_messages['messages'].sample(1,ignore_index=True)}\")\n",
    "\n",
    "df_medium_messages = df_full_dataset[(df_full_dataset['word_count']<=medium) & (df_full_dataset['word_count']>=short)]\n",
    "print(\"\\n------------------------------------------------------\\n\")\n",
    "print(f\"Length of medium messages dataset: {len(df_medium_messages)}\\n\")\n",
    "print(f\"Example of a message: {df_medium_messages['messages'].sample(1,ignore_index=True)}\")\n",
    "\n",
    "df_long_messages = df_full_dataset[df_full_dataset['word_count']>medium]\n",
    "print(\"\\n------------------------------------------------------\\n\")\n",
    "print(f\"Length of long messages dataset: {len(df_long_messages)}\\n\")\n",
    "print(f\"Example of a long message: {df_long_messages['messages'].sample(1,ignore_index=True)}\")\n",
    "\n",
    "#Reset df options\n",
    "pd.reset_option('display.max_colwidth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset accroding to sequence size for experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the length of messages == length of labels: True\n",
      "Checking the length of messages == length of labels: True\n",
      "Checking the length of messages == length of labels: True\n"
     ]
    }
   ],
   "source": [
    "#Extracting lists from df of short messages and checking length are compatible\n",
    "short_list=df_short_messages['messages'].to_list()\n",
    "short_list_labels=df_short_messages['labels'].to_list()\n",
    "print(f\"Checking the length of messages == length of labels: {len(short_list)==len(short_list_labels)}\")\n",
    "\n",
    "#Extracting lists from df of medium messages and checking length are compatible\n",
    "m_list=df_medium_messages['messages'].to_list()\n",
    "m_list_labels=df_medium_messages['labels'].to_list()\n",
    "print(f\"Checking the length of messages == length of labels: {len(m_list)==len(m_list_labels)}\")\n",
    "\n",
    "#Extracting lists from df of long messages and checking length are compatible\n",
    "l_list=df_long_messages['messages'].to_list()\n",
    "l_list_labels=df_long_messages['labels'].to_list()\n",
    "print(f\"Checking the length of messages == length of labels: {len(l_list)==len(l_list_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#Creating numpy arrays\n",
    "full_text_short = np.asarray(short_list)\n",
    "labels_short = np.asarray(short_list_labels)\n",
    "\n",
    "full_text_medium = np.asarray(m_list)\n",
    "labels_medium = np.asarray(m_list_labels)\n",
    "\n",
    "full_text_long = np.asarray(l_list)\n",
    "labels_long = np.asarray(l_list_labels)\n",
    "print(type(labels_long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since the full test data has ~1600 entries, I will not further split the created sets into train/test\n",
    "#The current sets are ~1800 so it's comparable to the original full test set\n",
    "\n",
    "#Steps as above to vectorize the sequences of words\n",
    "\n",
    "def vectorize_data(x,y):\n",
    "    # Tokenizning\n",
    "    tokenizer = Tokenizer()\n",
    "    fitted = tokenizer.fit_on_texts(x)\n",
    "    sequences = tokenizer.texts_to_sequences(x)\n",
    "\n",
    "    # after, stop putting more words\n",
    "    max_len = 500\n",
    "\n",
    "    idx_word = tokenizer.word_index\n",
    "    print(\"Tokens size: \", len(idx_word))\n",
    "\n",
    "    padded = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "    # Checking the shape\n",
    "    print(\"Data size after tokenizing: \", padded.shape)\n",
    "\n",
    "    np.random.seed(42)\n",
    "    indices = np.arange(padded.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    padded_sequences_idx = padded[indices]\n",
    "    labels = y[indices]\n",
    "\n",
    "    x_data = padded_sequences_idx[:]\n",
    "    y_data = labels[:]\n",
    "\n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens size:  2357\n",
      "Data size after tokenizing:  (1861, 500)\n",
      "Tokens size:  3846\n",
      "Data size after tokenizing:  (1843, 500)\n",
      "Tokens size:  6659\n",
      "Data size after tokenizing:  (1868, 500)\n"
     ]
    }
   ],
   "source": [
    "x_test_short, y_test_short = vectorize_data(full_text_short, labels_short)\n",
    "x_test_medium, y_test_medium = vectorize_data(full_text_medium, labels_medium)\n",
    "x_test_long, y_test_long = vectorize_data(full_text_long, labels_long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "122/122 [==============================] - 14s 101ms/step - loss: 0.2278 - acc: 0.9208\n",
      "Epoch 2/15\n",
      "122/122 [==============================] - 13s 106ms/step - loss: 0.0577 - acc: 0.9831\n",
      "Epoch 3/15\n",
      "122/122 [==============================] - 13s 104ms/step - loss: 0.0397 - acc: 0.9892\n",
      "Epoch 4/15\n",
      "122/122 [==============================] - 13s 105ms/step - loss: 0.0271 - acc: 0.9923\n",
      "Epoch 5/15\n",
      "122/122 [==============================] - 13s 105ms/step - loss: 0.0136 - acc: 0.9956\n",
      "Epoch 6/15\n",
      "122/122 [==============================] - 13s 107ms/step - loss: 0.0097 - acc: 0.9969\n",
      "Epoch 7/15\n",
      "122/122 [==============================] - 13s 109ms/step - loss: 0.0065 - acc: 0.9987\n",
      "Epoch 8/15\n",
      "122/122 [==============================] - 13s 108ms/step - loss: 0.0033 - acc: 0.9992\n",
      "Epoch 9/15\n",
      "122/122 [==============================] - 13s 106ms/step - loss: 0.0019 - acc: 0.9995\n",
      "Epoch 10/15\n",
      "122/122 [==============================] - 13s 104ms/step - loss: 0.0017 - acc: 0.9995\n",
      "Epoch 11/15\n",
      "122/122 [==============================] - 13s 104ms/step - loss: 8.1505e-04 - acc: 0.9995\n",
      "Epoch 12/15\n",
      "122/122 [==============================] - 13s 105ms/step - loss: 3.0906e-04 - acc: 1.0000\n",
      "Epoch 13/15\n",
      "122/122 [==============================] - 13s 105ms/step - loss: 3.7666e-04 - acc: 1.0000\n",
      "Epoch 14/15\n",
      "122/122 [==============================] - 13s 106ms/step - loss: 2.5793e-04 - acc: 1.0000\n",
      "Epoch 15/15\n",
      "122/122 [==============================] - 13s 108ms/step - loss: 1.7114e-04 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Vanilla test RNN model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 64))\n",
    "model.add(SimpleRNN(64,input_shape=x_train.shape,return_sequences=False, activation='tanh'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer ='rmsprop',metrics=['acc'])\n",
    "\n",
    "model_rnn = model.fit(x_train, y_train, epochs = 15, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of first model_rnn\n",
    "\n",
    "After running the very first model, it was observed that the model is probably overfitting the data. The accuracy after the epoch 15 is 100% and the model probably learns the noise. The model is not learning objectively and is fit too much to the training data. Before I actually start runnig the 3 models (vanilla RNN, LSTM and GRU), I will treat the model overfitting:\n",
    "* reducing the network's capacity --> decreasing the number of units in the model's layers\n",
    "* applying regularization technique --> penalizing very large weights\n",
    "* changing optimizer to Adam\n",
    "* increasing the batch size --> the model will learn lesser noise; it will help take a more reasonable 'step' for minima\n",
    "* adding dropout layers --> by dropping some layer of the network, I will let the model generalize better on unseen examples; it won't be trained for details and noises of the training data\n",
    "\n",
    "I will call this model a test model, apply the mentioned changes and start building the 3 RNN models for the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "16/16 [==============================] - 7s 161ms/step - loss: 0.8031 - acc: 0.6656 - precision_3: 0.1346 - recall_3: 0.2759\n",
      "Epoch 2/15\n",
      "16/16 [==============================] - 3s 168ms/step - loss: 0.6366 - acc: 0.8669 - precision_3: 1.0000 - recall_3: 0.0057\n",
      "Epoch 3/15\n",
      "16/16 [==============================] - 3s 160ms/step - loss: 0.5296 - acc: 0.8662 - precision_3: 0.0000e+00 - recall_3: 0.0000e+00\n",
      "Epoch 4/15\n",
      "16/16 [==============================] - 2s 154ms/step - loss: 0.4541 - acc: 0.8828 - precision_3: 1.0000 - recall_3: 0.1245\n",
      "Epoch 5/15\n",
      "16/16 [==============================] - 2s 154ms/step - loss: 0.3370 - acc: 0.9741 - precision_3: 0.9976 - recall_3: 0.8084\n",
      "Epoch 6/15\n",
      "16/16 [==============================] - 2s 154ms/step - loss: 0.2595 - acc: 0.9856 - precision_3: 0.9936 - recall_3: 0.8985\n",
      "Epoch 7/15\n",
      "16/16 [==============================] - 3s 172ms/step - loss: 0.2240 - acc: 0.9887 - precision_3: 0.9938 - recall_3: 0.9215\n",
      "Epoch 8/15\n",
      "16/16 [==============================] - 2s 150ms/step - loss: 0.2056 - acc: 0.9903 - precision_3: 0.9919 - recall_3: 0.9349\n",
      "Epoch 9/15\n",
      "16/16 [==============================] - 3s 156ms/step - loss: 0.2202 - acc: 0.9790 - precision_3: 0.8986 - recall_3: 0.9502\n",
      "Epoch 10/15\n",
      "16/16 [==============================] - 3s 167ms/step - loss: 0.2723 - acc: 0.9469 - precision_3: 0.9968 - recall_3: 0.6054\n",
      "Epoch 11/15\n",
      "16/16 [==============================] - 2s 153ms/step - loss: 0.2523 - acc: 0.9597 - precision_3: 1.0000 - recall_3: 0.6992\n",
      "Epoch 12/15\n",
      "16/16 [==============================] - 2s 154ms/step - loss: 0.2068 - acc: 0.9821 - precision_3: 1.0000 - recall_3: 0.8659\n",
      "Epoch 13/15\n",
      "16/16 [==============================] - 3s 158ms/step - loss: 0.1914 - acc: 0.9903 - precision_3: 1.0000 - recall_3: 0.9272\n",
      "Epoch 14/15\n",
      "16/16 [==============================] - 2s 152ms/step - loss: 0.1849 - acc: 0.9921 - precision_3: 0.9980 - recall_3: 0.9425\n",
      "Epoch 15/15\n",
      "16/16 [==============================] - 2s 154ms/step - loss: 0.1806 - acc: 0.9936 - precision_3: 0.9980 - recall_3: 0.9540\n",
      "---------------------------------------------------\n",
      "Model evaluation on the test set:\n",
      "53/53 [==============================] - 3s 26ms/step - loss: 0.2303 - acc: 0.9767 - precision_3: 0.9947 - recall_3: 0.8311\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': 0.23030498623847961,\n",
       " 'acc': 0.9766746163368225,\n",
       " 'precision_3': 0.9946808218955994,\n",
       " 'recall_3': 0.8311111330986023}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vanilla RNN model 1 \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 15))\n",
    "model.add(SimpleRNN(15,input_shape=x_train.shape, activation='tanh',recurrent_regularizer='l2'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics=['acc', Precision(), Recall()])\n",
    "\n",
    "model_vanilla_rnn = model.fit(x_train, y_train, epochs = 15, batch_size=256)\n",
    "print(\"---------------------------------------------------\")\n",
    "print(\"Model evaluation on the full test set:\")\n",
    "model.evaluate(x_test,y_test, verbose=1, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - 9s 310ms/step - loss: 0.8018 - acc: 0.8449\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 5s 282ms/step - loss: 0.6826 - acc: 0.8662\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 4s 276ms/step - loss: 0.4927 - acc: 0.8662\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 4s 276ms/step - loss: 0.3997 - acc: 0.8662\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 5s 318ms/step - loss: 0.3187 - acc: 0.8900\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 5s 333ms/step - loss: 0.2515 - acc: 0.9341\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 5s 319ms/step - loss: 0.2042 - acc: 0.9667\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 5s 337ms/step - loss: 0.1733 - acc: 0.9790\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 5s 309ms/step - loss: 0.1504 - acc: 0.9838\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 5s 307ms/step - loss: 0.1316 - acc: 0.9882\n"
     ]
    }
   ],
   "source": [
    "# Vanilla LSTM model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 15))\n",
    "model.add(LSTM(15,input_shape=x_train.shape, activation='tanh',recurrent_regularizer='l2'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics=['acc'])\n",
    "\n",
    "model_lstm = model.fit(x_train, y_train, epochs = 10, batch_size=256)\n",
    "model.evaluate(x_test,y_test, verbose=1, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - 10s 281ms/step - loss: 0.8059 - acc: 0.8336\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 4s 280ms/step - loss: 0.7132 - acc: 0.8662\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 4s 278ms/step - loss: 0.5864 - acc: 0.8662\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 5s 285ms/step - loss: 0.4391 - acc: 0.8662\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 5s 288ms/step - loss: 0.3587 - acc: 0.8674\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 5s 286ms/step - loss: 0.2896 - acc: 0.9082\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 5s 282ms/step - loss: 0.2279 - acc: 0.9503\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 5s 283ms/step - loss: 0.1799 - acc: 0.9687\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 5s 284ms/step - loss: 0.1464 - acc: 0.9821\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 5s 288ms/step - loss: 0.1230 - acc: 0.9874\n"
     ]
    }
   ],
   "source": [
    "# Vanilla LSTM model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 15))\n",
    "model.add(GRU(15,input_shape=x_train.shape, activation='tanh',recurrent_regularizer='l2'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics=['acc'])\n",
    "\n",
    "model_gru = model.fit(x_train, y_train, epochs = 10, batch_size=256)\n",
    "model.evaluate(x_test,y_test, verbose=1, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "85b31755cbf75356c393a3522367cd288f0b05170e2bd292c75b11fc3d2da2cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
